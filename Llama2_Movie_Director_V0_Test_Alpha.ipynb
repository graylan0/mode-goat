{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOVa6mfvjPpBOuoJk0qEmH3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/graylan0/mode-goat/blob/main/Llama2_Movie_Director_V0_Test_Alpha.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi\n",
        "!pip install nest-asyncio\n",
        "!pip install json\n",
        "!pip install uvicorn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrssuDN4JC8k",
        "outputId": "1431f63a-90b7-4e72-8b12-9bc683d11708"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (0.103.1)\n",
            "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi) (3.7.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (1.10.12)\n",
            "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (0.27.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi) (1.1.3)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (1.5.7)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement json (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for json\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (0.23.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (4.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/TheBloke/Llama-2-7B-GGML/resolve/main/llama-2-7b.ggmlv3.q8_0.bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pdi_TrGaKPfN",
        "outputId": "1b753fdc-b6d1-4835-b88b-8a404094f29d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-11 07:34:12--  https://huggingface.co/TheBloke/Llama-2-7B-GGML/resolve/main/llama-2-7b.ggmlv3.q8_0.bin\n",
            "Resolving huggingface.co (huggingface.co)... 108.138.94.97, 108.138.94.52, 108.138.94.27, ...\n",
            "Connecting to huggingface.co (huggingface.co)|108.138.94.97|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/f8/8d/f88de0c18630cb860e75d1dc208c88baf212ec49bb6ff8b476801b0901ced546/5bb0702855f0c8abc645ea68c4d41e05207964ff54dd38c2787c1c1206cae121?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-7b.ggmlv3.q8_0.bin%3B+filename%3D%22llama-2-7b.ggmlv3.q8_0.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1694676852&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5NDY3Njg1Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9mOC84ZC9mODhkZTBjMTg2MzBjYjg2MGU3NWQxZGMyMDhjODhiYWYyMTJlYzQ5YmI2ZmY4YjQ3NjgwMWIwOTAxY2VkNTQ2LzViYjA3MDI4NTVmMGM4YWJjNjQ1ZWE2OGM0ZDQxZTA1MjA3OTY0ZmY1NGRkMzhjMjc4N2MxYzEyMDZjYWUxMjE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=gGSImmNw-rziNCdus919c5f5Vdsb78f2S1qK1-Ohgx3YSaiuUL1w4SUOtzbkaScuCwcfuxaTlr5lVtZY7d0IKCq0aFKz5yVj5D4n6zl3AHs8VITfY-Fkdu5cw5ywkkbeMdU7G9ImuOHSSgxyfal6soxFuM6EyVWzi1EXT-7xJcrzsRVOgUlw6xBecfkMRcMFlKC%7ETKuQcJkf8NCACyOO4tfG-HRrjNP59epiJQRjg9XjnZiS9pnZexTP3MdsFuW7lH%7Ec1P8BVKOr-R8CDNiLm3ehz-4Zyk%7Ed2nWUm9bQDzy567Kqrvw2OHJOhzwaj-RLqe%7EM5DDx80KiPFxJcTOx-g__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-09-11 07:34:12--  https://cdn-lfs.huggingface.co/repos/f8/8d/f88de0c18630cb860e75d1dc208c88baf212ec49bb6ff8b476801b0901ced546/5bb0702855f0c8abc645ea68c4d41e05207964ff54dd38c2787c1c1206cae121?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-7b.ggmlv3.q8_0.bin%3B+filename%3D%22llama-2-7b.ggmlv3.q8_0.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1694676852&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5NDY3Njg1Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9mOC84ZC9mODhkZTBjMTg2MzBjYjg2MGU3NWQxZGMyMDhjODhiYWYyMTJlYzQ5YmI2ZmY4YjQ3NjgwMWIwOTAxY2VkNTQ2LzViYjA3MDI4NTVmMGM4YWJjNjQ1ZWE2OGM0ZDQxZTA1MjA3OTY0ZmY1NGRkMzhjMjc4N2MxYzEyMDZjYWUxMjE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=gGSImmNw-rziNCdus919c5f5Vdsb78f2S1qK1-Ohgx3YSaiuUL1w4SUOtzbkaScuCwcfuxaTlr5lVtZY7d0IKCq0aFKz5yVj5D4n6zl3AHs8VITfY-Fkdu5cw5ywkkbeMdU7G9ImuOHSSgxyfal6soxFuM6EyVWzi1EXT-7xJcrzsRVOgUlw6xBecfkMRcMFlKC%7ETKuQcJkf8NCACyOO4tfG-HRrjNP59epiJQRjg9XjnZiS9pnZexTP3MdsFuW7lH%7Ec1P8BVKOr-R8CDNiLm3ehz-4Zyk%7Ed2nWUm9bQDzy567Kqrvw2OHJOhzwaj-RLqe%7EM5DDx80KiPFxJcTOx-g__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 52.84.162.75, 52.84.162.32, 52.84.162.46, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|52.84.162.75|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7160799872 (6.7G) [application/octet-stream]\n",
            "Saving to: ‘llama-2-7b.ggmlv3.q8_0.bin’\n",
            "\n",
            "llama-2-7b.ggmlv3.q 100%[===================>]   6.67G  37.0MB/s    in 2m 37s  \n",
            "\n",
            "2023-09-11 07:36:50 (43.5 MB/s) - ‘llama-2-7b.ggmlv3.q8_0.bin’ saved [7160799872/7160799872]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8S_ckwDyI2LV",
        "outputId": "18a0110e-54d1-41da-8b7c-4c8efee75c22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python==0.1.78\n",
            "  Downloading llama_cpp_python-0.1.78.tar.gz (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (1.23.5)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python==0.1.78)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.78-cp310-cp310-linux_x86_64.whl size=5822302 sha256=5bf25477ea5e670a5ddc8cda8920435365d45369e94b072fd1df10b3679ae36e\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/f9/20/9ca660a9d3f2a47e44217059409478865948b5c8a1cba70030\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.1.78\n"
          ]
        }
      ],
      "source": [
        "# Build Llama cpp\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.78"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import json\n",
        "import logging\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from llama_cpp import Llama  # Assuming you have this package installed\n",
        "\n",
        "# Initialize logging to debug level to capture detailed logs\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "\n",
        "llm = Llama(\n",
        "  model_path=\"llama-2-7b.ggmlv3.q8_0.bin\",\n",
        "  n_gpu_layers=24,\n",
        "  n_ctx=4000,\n",
        ")\n",
        "\n",
        "executor = ThreadPoolExecutor(max_workers=3)\n",
        "\n",
        "# Function to trim tokens in a string to fit within a given limit\n",
        "def trim_tokens(text, max_tokens):\n",
        "    tokens = text.split()\n",
        "    while len(tokens) > max_tokens:\n",
        "        tokens.pop(0)\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Function to extract key topics from a frame\n",
        "def extract_key_topics(frame):\n",
        "    return ' '.join(frame.split()[-3:])\n",
        "\n",
        "async def continue_next_frame_generation(last_three_frames):\n",
        "    key_topics = [extract_key_topics(frame) for frame in last_three_frames]\n",
        "    combined_key_topics = ' '.join(key_topics)\n",
        "    rules_prompt = f\"As an AI specialized in Advanced Space Movies, you are tasked with generating a scene description based on these key topics: {combined_key_topics}\"\n",
        "    new_frame_generation = llm(rules_prompt, max_tokens=198)['choices'][0]['text']\n",
        "    return new_frame_generation\n",
        "\n",
        "async def generate_advanced_space_scene():\n",
        "    rules_prompt = \"As an AI specialized in Advanced Space Movies, generate an 18-word description of an advanced space scene.\"\n",
        "    scene_output = llm(rules_prompt, max_tokens=150)['choices'][0]['text']\n",
        "    return scene_output\n",
        "\n",
        "async def start_movie(topic):\n",
        "    initial_prompt = llm(f\"Create a writing story prompt to start a Multiverse Movie Generator Game about {topic}.\", max_tokens=312)['choices'][0]['text']\n",
        "    frames = {}\n",
        "    frames[\"frame_0\"] = initial_prompt\n",
        "    last_three_frames = [initial_prompt, \"\", \"\"]\n",
        "\n",
        "    for i in range(1, 3):\n",
        "        advanced_space_scene = await generate_advanced_space_scene()\n",
        "        new_frame_generation = await continue_next_frame_generation([initial_prompt, advanced_space_scene, \"\"])\n",
        "        frames[f\"frame_{i}\"] = new_frame_generation\n",
        "        last_three_frames.pop(0)\n",
        "        last_three_frames.append(new_frame_generation)\n",
        "\n",
        "    sanitized_topic = ''.join(e for e in topic if e.isalnum())\n",
        "    sanitized_topic = sanitized_topic[:50]\n",
        "\n",
        "    with open(f\"{sanitized_topic}_movie_frames.json\", \"w\") as f:\n",
        "        json.dump(frames, f, indent=4)\n",
        "\n",
        "    return f\"Advanced Space Movie about {topic} started and 500 frames generated. Saved to {sanitized_topic}_movie_frames.json\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(start_movie(\"StarshipTroopersMovie\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqqQ0d6PI-1R",
        "outputId": "155675da-c5a7-4100-bb48-31457a08940f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n"
          ]
        }
      ]
    }
  ]
}